/* Auto-generated from /resources. Do not edit manually. */

export const content = {
  "profile": {
    "name": "Chintan Goyal",
    "title": "Senior Data and Platform Engineer",
    "tagline": "Architecting data platforms at scale. Systems thinker with a bias for rigor.",
    "bio": "I design and scale data platforms that power real-time decisions for millions of devices. My focus is on distributed systems, reliability, and cost-aware architecture with measurable outcomes.",
    "location": "Hyderabad, India",
    "timezone": "Asia/Kolkata (IST)",
    "company": "Plume Design Inc",
    "email": "mail.chintan.goyal@gmail.com",
    "phone": "+91 9462010301",
    "linkedin": "https://linkedin.com/chintan-goyal",
    "roles": [
      "Data Engineer",
      "Platform Architect",
      "Systems Thinker",
      "Future CTO"
    ],
    "social": {
      "github": "https://github.com/goyal-chintan",
      "linkedin": "https://linkedin.com/chintan-goyal",
      "twitter": "https://x.com/gchintn",
      "email": "mail.chintan.goyal@gmail.com",
      "calendar": "",
      "website": ""
    },
    "availability": {
      "status": "open",
      "note": "Open to ambitious data and platform challenges."
    },
    "stats": [
      {
        "label": "Devices",
        "value": "50M+"
      },
      {
        "label": "Data Footprint",
        "value": "50PB"
      },
      {
        "label": "Cost Saved",
        "value": "2M USD"
      },
      {
        "label": "Customers Enabled",
        "value": "15M"
      }
    ],
    "focus_areas": [
      "Distributed systems",
      "Data platforms and pipelines",
      "Cloud reliability and observability",
      "Performance and cost optimization",
      "System design and architecture"
    ],
    "brand_keywords": [
      "deep thinking",
      "systems",
      "clarity",
      "rigor",
      "precision"
    ],
    "about": {
      "headline": "The operating system behind the engineering.",
      "current_focus": "Scaling distributed data platforms and architecting systems that explain themselves.",
      "lifestyle": [
        {
          "id": "fuel",
          "title": "Fuel",
          "value": "Ethiopian Yirgacheffe",
          "detail": "V60 Pour Over",
          "why": "Ritual + repeatability. A slow start reduces context switching and sharpens decisions."
        },
        {
          "id": "input",
          "title": "Input",
          "value": "Keychron Q1 Pro",
          "detail": "Mechanical tactility",
          "why": "Tactile feedback keeps me deliberate under load—fewer mistakes, lower fatigue in long sessions."
        },
        {
          "id": "audio",
          "title": "Audio",
          "value": "Sony WH-1000XM5",
          "detail": "Deep focus",
          "why": "Noise cancellation creates a controlled environment for deep work and design reviews."
        },
        {
          "id": "center",
          "title": "Center",
          "value": "Vipassana",
          "detail": "The internal OS",
          "why": "Attention training shows up in incidents and tradeoffs: calmer response, clearer reasoning."
        }
      ],
      "journey": [
        {
          "period": "2023 — Present",
          "role": "Senior Data & Platform Engineer",
          "company": "Plume Design Inc",
          "active": true
        },
        {
          "period": "2021 — 2023",
          "role": "Senior Data & Platform Engineer",
          "company": "Morgan Stanley"
        },
        {
          "period": "2019 — 2021",
          "role": "Data Engineer",
          "company": "Fractal Analytics"
        },
        {
          "period": "2017 — 2019",
          "role": "Data Engineer",
          "company": "Infosys"
        }
      ]
    }
  },
  "nav": {
    "primary": [
      {
        "id": "home",
        "label": "Home",
        "href": "/",
        "type": "route"
      },
      {
        "id": "contact",
        "label": "Contact",
        "href": "/#contact",
        "type": "scroll"
      }
    ],
    "theme_toggle": true
  },
  "resume": {
    "summary": "Senior Data & Platform Engineer focused on multi-cloud architectures, scalable data platforms, and cost-aware systems design.",
    "experience": [
      {
        "company": "Plume Design Inc",
        "role": "Senior Data & Platform Engineer",
        "period": "Aug 2023 - Present",
        "highlights": [
          "Led multi-cloud architecture to onboard ISP customers on GCP while maintaining AWS parity; enabled 15M customers.",
          "Reduced infrastructure cost by 30% and data quality issues by 50% by migrating YugabyteDB ingestion from Lambda to Spark Streaming at 40 GB/hour.",
          "Designed deep archival strategy for 50 PB of data; identified 20% cost reduction (~150k USD/month).",
          "Saved ~2M USD in Databricks costs by shifting workloads to EKS-based clusters and optimizing DBUs.",
          "Open-sourced Project Lumos: pluggable metadata and governance framework; saved 400+ hours/year and reduced bug triage effort across teams."
        ]
      },
      {
        "company": "Morgan Stanley",
        "role": "Senior Data & Platform Engineer",
        "period": "May 2021 - Jul 2023",
        "highlights": [
          "Built self-service data platform for global trade settlements; reduced reporting turnaround by 1.5 weeks.",
          "Improved OLTP SQL Server latency by 40% via hourly archival and optimized indexing on Type-2 SCD tables.",
          "Saved 120 hours/week by automating ownership assignment of trade fails using a Drools-based CQRS pipeline.",
          "Owned critical data warehouse across Snowflake/DB2/Sybase/CDC pipelines; saved 10M USD with governance and quality protocols.",
          "Led an innovation project using Neo4J knowledge graphs for fraud detection; 3rd place among 70 teams."
        ]
      },
      {
        "company": "Fractal Analytics",
        "role": "Data Engineer",
        "period": "Apr 2019 - May 2021",
        "highlights": [
          "Built COVID-19 India data platform with PMO/NITI AYOG/NASSCOM; improved prediction accuracy by 45% with district-level granularity.",
          "Designed serverless ingestion and modeling on AWS Athena/Glue/Lambda for national dashboards.",
          "Led cross-functional discussions with Mapbox and Infosys on privacy/security for movement data.",
          "Delivered a European health tech data platform; reduced infra cost by 40% and improved pipeline runtime 3x.",
          "Automated campaign analytics to save 24 hours/week across regions."
        ]
      },
      {
        "company": "Infosys Limited",
        "role": "Data Engineer",
        "period": "Mar 2017 - Apr 2019",
        "highlights": [
          "Developed SQL ETLs and Spark-Scala pipelines on 500 PB clusters; optimized batch Hive loads for reliability."
        ]
      }
    ],
    "skills": [
      {
        "category": "Programming",
        "items": [
          "Scala",
          "Python",
          "Java",
          "SQL",
          "Cypher",
          "Bash",
          "Perl"
        ]
      },
      {
        "category": "Data Engineering Frameworks",
        "items": [
          "Spark",
          "Databricks",
          "Hadoop",
          "EMR",
          "EC2",
          "Lambda",
          "MapReduce",
          "Spark MLlib",
          "YARN",
          "Sqoop",
          "Kafka",
          "GCP Dataproc",
          "BigQuery",
          "Pub/Sub"
        ]
      },
      {
        "category": "Databases and Table Formats",
        "items": [
          "Hive",
          "Snowflake",
          "Delta Lake",
          "Glue",
          "Athena",
          "Neo4J",
          "DB2",
          "Sybase",
          "SQL Server",
          "MySQL",
          "Yugabyte"
        ]
      },
      {
        "category": "Orchestration and DevOps",
        "items": [
          "Step Functions",
          "Shell Scripting",
          "Git",
          "Autosys",
          "Liquibase",
          "Jenkins",
          "Jira",
          "Azure DevOps",
          "Bitbucket",
          "Airflow"
        ]
      },
      {
        "category": "Cloud, Storage, and Formats",
        "items": [
          "AWS",
          "Azure",
          "HDFS",
          "S3",
          "EBS",
          "ADLS",
          "Blob Storage",
          "Cloud Storage",
          "Parquet",
          "ORC",
          "Delta",
          "Avro",
          "JSON",
          "XML",
          "CSV",
          "Columnar",
          "Linux"
        ]
      }
    ],
    "awards": [
      "Outstanding Delivery, Morgan Stanley",
      "3rd Place, Technology Innovation Program, Morgan Stanley",
      "Corona Warrior, NASSCOM",
      "S.M.A.R.T Award, end-to-end Solution, Fractal"
    ],
    "education": [
      {
        "program": "Master of Science in Computer Science",
        "institution": "Woolf University",
        "period": "2023 - 2025 (ongoing)",
        "notes": "GPA 4.0, average marks 97%"
      },
      {
        "program": "Bachelor of Technology in Mechanical Engineering",
        "institution": "Rajasthan Technical University",
        "period": "2012 - 2016",
        "notes": ""
      }
    ]
  },
  "stack": {
    "categories": [
      {
        "name": "Languages",
        "items": [
          "Scala",
          "Python",
          "Java",
          "SQL",
          "Cypher",
          "Bash",
          "Perl"
        ]
      },
      {
        "name": "Data Frameworks",
        "items": [
          "Spark",
          "Databricks",
          "Hadoop",
          "EMR",
          "EC2",
          "Lambda",
          "MapReduce",
          "Spark MLlib",
          "YARN",
          "Sqoop",
          "Kafka",
          "GCP Dataproc",
          "BigQuery",
          "Pub/Sub"
        ]
      },
      {
        "name": "Databases and Table Formats",
        "items": [
          "Hive",
          "Snowflake",
          "Delta Lake",
          "Glue",
          "Athena",
          "Neo4J",
          "DB2",
          "Sybase",
          "SQL Server",
          "MySQL",
          "Yugabyte"
        ]
      },
      {
        "name": "Orchestration and DevOps",
        "items": [
          "Step Functions",
          "Shell Scripting",
          "Git",
          "Autosys",
          "Liquibase",
          "Jenkins",
          "Jira",
          "Azure DevOps",
          "Bitbucket",
          "Airflow"
        ]
      },
      {
        "name": "Cloud, Storage, and Formats",
        "items": [
          "AWS",
          "Azure",
          "HDFS",
          "S3",
          "EBS",
          "ADLS",
          "Blob Storage",
          "Cloud Storage",
          "Parquet",
          "ORC",
          "Delta",
          "Avro",
          "JSON",
          "XML",
          "CSV",
          "Columnar",
          "Linux"
        ]
      }
    ]
  },
  "library": {
    "books": []
  },
  "thoughts": {
    "thoughts": []
  },
  "projects": [
    {
      "id": "cost-optimization-50pb",
      "name": "50PB Cost Optimization Program",
      "period": "2023-2024",
      "company": "Plume Design Inc",
      "status": "production",
      "open_source": false,
      "link_status": "ready",
      "link": {
        "primary": {
          "type": "resume",
          "label": "See in resume",
          "url": "/resume"
        }
      },
      "privacy_note": "Private case study. Details on request.",
      "tags": [
        "Cost optimization",
        "Storage",
        "Compute",
        "Data lifecycle"
      ],
      "metrics": [
        {
          "label": "Monthly savings",
          "value": "150k USD"
        },
        {
          "label": "Databricks savings",
          "value": "2M USD"
        }
      ],
      "summary": "Deep archival strategy and compute portability for 50PB of data with substantial monthly savings.",
      "body": "## Context\nCloud spend was rising with scale. We needed a long-term data lifecycle strategy without degrading access or reliability.\n\n## Constraints\n- 50PB of data across multiple workloads.\n- Business expectations for access, latency, and compliance.\n- Cost cuts must not reduce quality or resiliency.\n\n## What I built\n- Deep archival strategy for cold data.\n- Workload right-sizing and cluster optimization.\n- Migration of select workloads off Databricks to EKS-based clusters.\n\n## Impact\n- Identified and realized ~20% cost reduction, about 150k USD per month.\n- Saved ~2M USD in Databricks costs.\n\n## Stack\nCloud storage tiers, EKS-based compute, and cost governance workflows.",
      "file": "cost-optimization-50pb.md"
    },
    {
      "id": "covid-platform",
      "name": "COVID-19 India Data Platform",
      "period": "2019-2021",
      "company": "Fractal Analytics",
      "status": "production",
      "open_source": false,
      "link_status": "ready",
      "link": {
        "primary": {
          "type": "resume",
          "label": "See in resume",
          "url": "/resume"
        }
      },
      "privacy_note": "Private case study. Details on request.",
      "tags": [
        "Public sector",
        "Serverless",
        "Data modeling",
        "Ingestion"
      ],
      "metrics": [
        {
          "label": "Prediction improvement",
          "value": "45%"
        }
      ],
      "summary": "Serverless data platform powering national dashboards with improved prediction accuracy and data coverage.",
      "body": "## Context\nNational and state dashboards needed reliable, near real-time data for hospital readiness and virus spread.\n\n## Constraints\n- High data volume and rapid change.\n- Sensitive data handling and privacy constraints.\n- Coordination across multiple stakeholders.\n\n## What I built\n- Serverless ingestion and processing using AWS services.\n- Data modeling and warehousing for analytics.\n- Governance and access controls for public sector requirements.\n\n## Impact\n- Improved prediction model outputs by 45% via finer-grained data ingestion.\n- Supported dashboards for central and state governments.\n\n## Stack\nAWS Athena, Glue, Lambda, and serverless ingestion pipelines.",
      "file": "covid-platform.md"
    },
    {
      "id": "lumos",
      "name": "Project Lumos",
      "period": "2023-2024",
      "company": "Plume Design Inc",
      "status": "production",
      "open_source": true,
      "link_status": "pending",
      "link": {
        "primary": {
          "type": "resume",
          "label": "See in resume",
          "url": "/resume"
        }
      },
      "privacy_note": "Open source repo link pending.",
      "tags": [
        "Data governance",
        "Metadata",
        "Lineage",
        "Discovery"
      ],
      "metrics": [
        {
          "label": "Effort saved",
          "value": "400+ hours per year"
        },
        {
          "label": "Engineers unblocked",
          "value": "10 FTE"
        }
      ],
      "summary": "Pluggable, tech-agnostic metadata and data governance framework that unified scattered data assets.",
      "body": "## Context\nData assets were scattered across teams, with no single place for ownership, lineage, or documentation. Engineers lost time finding datasets and resolving data quality issues.\n\n## Constraints\n- Built as a side project with a small team on weekends.\n- Needed to work across different stacks and storage systems.\n- Adoption depended on low friction and clear value.\n\n## What I built\n- A metadata and cataloging layer for discovery and ownership.\n- Lineage and versioned assets for traceability.\n- A pluggable design so teams could integrate without rewrites.\n\n## Impact\n- Saved 400+ effort hours annually by reducing manual data discovery and bug triage.\n- Became the first-priority platform initiative after an internal demo.\n\n## Stack\nMetadata services, lineage pipelines, UI and API surfaces, and integration connectors.",
      "file": "lumos.md"
    },
    {
      "id": "multicloud-platform",
      "name": "Multi-Cloud Data Platform",
      "period": "2023-2024",
      "company": "Plume Design Inc",
      "status": "production",
      "open_source": false,
      "link_status": "ready",
      "link": {
        "primary": {
          "type": "resume",
          "label": "See in resume",
          "url": "/resume"
        }
      },
      "privacy_note": "Private case study. Details on request.",
      "tags": [
        "Multi-cloud",
        "Data platform",
        "Reliability",
        "Cost optimization"
      ],
      "metrics": [
        {
          "label": "Customers enabled",
          "value": "15M"
        },
        {
          "label": "Cost reduction",
          "value": "30%"
        },
        {
          "label": "Data quality",
          "value": "50% fewer issues"
        }
      ],
      "summary": "Cloud-agnostic data platform architecture to onboard large ISPs on GCP while maintaining AWS parity.",
      "body": "## Context\nWe needed to onboard large ISP customers on GCP while keeping the existing AWS platform reliable and consistent. The platform had to be cloud-agnostic without breaking existing pipelines.\n\n## Constraints\n- Zero downtime migration for critical data flows.\n- Different cloud primitives and cost models.\n- High throughput ingestion and strict data quality expectations.\n\n## What I built\n- Multi-cloud architecture and deployment strategy for the data platform.\n- Migration paths for streaming and batch workloads.\n- Governance and reliability guardrails to keep parity across clouds.\n\n## Impact\n- Enabled onboarding for ISPs with 15M customers.\n- Reduced infrastructure cost by 30%.\n- Cut data quality issues by 50%.\n\n## Stack\nGCP and AWS, streaming pipelines, and platform-level abstractions for portability.",
      "file": "multicloud-platform.md"
    }
  ],
  "writing": [
    {
      "id": "cost-optimization-50pb",
      "status": "draft",
      "title": "50PB and the Cost Curve: A Practical Playbook",
      "date": "2025-01-08",
      "read_time": "10 min",
      "tags": [
        "Cost optimization",
        "Storage",
        "Data lifecycle",
        "Cloud"
      ],
      "summary": "A practical approach to reducing cloud spend without breaking reliability or access.",
      "body": "## Executive summary\nAt scale, cost is architecture. I led a cost optimization program across 50PB of data by designing a deep archival strategy, right-sizing compute, and migrating select workloads from Databricks to EKS. The result: ~20% savings, roughly 150k USD per month, plus ~2M USD in Databricks savings.\n\n## Why this mattered\nAs data volume grows, default storage and compute choices become financially dangerous. We needed a path that preserved access and compliance while improving unit economics.\n\n## Constraints and signals\n- 50PB data footprint across multiple pipelines.\n- SLA requirements for access and recovery.\n- Org-wide budgets with clear monthly targets.\n\n## Architecture (high level)\n- Tiered storage with deep archival for cold data.\n- Policy-driven lifecycle rules tied to data criticality.\n- Compute portability for workloads that did not require Databricks.\n\n## Decisions and tradeoffs\n- We traded some convenience for predictable savings by enforcing tiering policies.\n- We kept a small buffer of hot data to avoid operational surprises.\n- We treated cost deltas as design constraints during change reviews.\n\n## What worked\n- Lifecycle rules stopped uncontrolled growth without disrupting teams.\n- Workload profiling uncovered surprising candidates for low-cost runtimes.\n- Cost visibility improved decision quality across engineering and finance.\n\n## What I would change\n- I would create a pre-built cost simulation model earlier to accelerate decision cycles.\n\n## Key takeaways\n- Cost optimization is a system, not a one-time cleanup.\n- Storage policies and compute portability are the biggest levers at scale.\n- The best savings come from structural changes, not discounts.",
      "file": "cost-optimization-50pb.md"
    },
    {
      "id": "multicloud-architecture",
      "status": "draft",
      "title": "Designing a Multi-Cloud Data Platform for 15M Customers",
      "date": "2025-01-05",
      "read_time": "12 min",
      "tags": [
        "Multi-cloud",
        "Data platforms",
        "Reliability",
        "Architecture"
      ],
      "summary": "How we made a data platform cloud-agnostic without breaking throughput, quality, or cost targets.",
      "body": "## Executive summary\nI led the architecture for a cloud-agnostic data platform so we could onboard large ISP customers on GCP while keeping AWS parity. The result: 15M customers onboarded, 30% infrastructure cost reduction, and 50% fewer data quality issues.\n\n## Why this mattered\nWe had a proven AWS platform with large-scale pipelines. A major ISP required GCP. We needed to support both clouds without creating two separate platforms or risking downtime.\n\n## Constraints and signals\n- Zero downtime for critical pipelines.\n- Ingestion volumes at 40 GB per hour, with strict latency and quality SLAs.\n- Cost targets and compliance requirements in two different cloud ecosystems.\n\n## Architecture (high level)\n- A cloud-agnostic platform layer that abstracts storage, compute, and orchestration differences.\n- Migration paths for streaming and batch workloads with parity testing.\n- Quality gates and lineage controls so both clouds produce identical outputs.\n\n## Decisions and tradeoffs\n- We prioritized portability over vendor-specific optimizations to reduce long-term operational risk.\n- We accepted slight performance overhead in exchange for consistent behavior across clouds.\n- We used a phased rollout with canary pipelines to validate parity.\n\n## What worked\n- Cloud-agnostic deployment patterns made onboarding faster and more predictable.\n- A clear cost-model comparison allowed us to select the best ingestion path for each workload.\n- Shared observability reduced the risk of hidden divergence between clouds.\n\n## What I would change\n- I would codify a portability test suite earlier to reduce manual checks.\n- I would invest sooner in automated cost telemetry per pipeline.\n\n## Key takeaways\n- Multi-cloud succeeds when the platform chooses consistency over cleverness.\n- A portability layer is only as strong as its validation system.\n- Treat cost as a first-class architectural signal, not a post-hoc metric.",
      "file": "multicloud-architecture.md"
    },
    {
      "id": "project-lumos",
      "status": "draft",
      "title": "Project Lumos: Building a Governance Framework That Teams Actually Use",
      "date": "2025-01-12",
      "read_time": "11 min",
      "tags": [
        "Data governance",
        "Metadata",
        "Lineage",
        "Platform"
      ],
      "summary": "How a weekend side project became a core platform for data discovery, lineage, and trust.",
      "body": "## Executive summary\nProject Lumos started as a side project to fix a recurring pain: data assets were scattered, ownership was unclear, and quality bugs took too long to resolve. We built a pluggable governance layer for discovery, lineage, and documentation that saved 400+ effort hours annually and became a first-priority platform initiative.\n\n## Why this mattered\nMost governance tools fail because they add friction. We needed something engineers would willingly use and that could work across multiple tech stacks without rewrites.\n\n## Constraints and signals\n- Small team, built outside core roadmap.\n- Needed to integrate with different storage and pipeline systems.\n- Adoption had to be obvious in the first week, not month three.\n\n## Architecture (high level)\n- Metadata services with versioned assets.\n- Lineage graphs linking sources, pipelines, and downstream consumers.\n- Lightweight UI and APIs for discovery and ownership.\n\n## Decisions and tradeoffs\n- We kept the data model intentionally minimal to encourage adoption.\n- We invested in connectors rather than a monolithic system rewrite.\n- We focused on accuracy of lineage over depth of visualization.\n\n## What worked\n- Teams started using it because it solved a real, daily problem.\n- It reduced time spent on bug ownership and data triage.\n- The architecture stayed flexible as new systems were added.\n\n## What I would change\n- I would automate metadata capture earlier to reduce manual steps.\n\n## Key takeaways\n- Governance succeeds when it removes friction, not when it adds process.\n- Lineage is a trust mechanism, not just a diagram.\n- The best platform tools are boring, reliable, and obvious.",
      "file": "project-lumos.md"
    },
    {
      "id": "streaming-ingestion",
      "status": "draft",
      "title": "Rebuilding Streaming Ingestion for Cost, Latency, and Quality",
      "date": "2025-01-15",
      "read_time": "9 min",
      "tags": [
        "Streaming",
        "Data platforms",
        "Cost optimization",
        "Reliability"
      ],
      "summary": "Why we moved from Lambda-based ingestion to Spark Streaming and how it reduced cost and data quality issues.",
      "body": "## Executive summary\nI redesigned a Lambda-based ingestion framework for YugabyteDB into a Spark Streaming pipeline that handled 40 GB per hour with better latency and higher data quality. The change reduced infrastructure cost by 30% and cut data quality issues by 50%.\n\n## Why this mattered\nThe existing ingestion approach was expensive at scale and produced inconsistent data quality across markets. We needed a streaming pipeline that could scale predictably and behave consistently under load.\n\n## Constraints and signals\n- 40 GB per hour ingestion load.\n- Cost and latency had to improve simultaneously.\n- Multiple market deployments with different cloud cost profiles.\n\n## Architecture (high level)\n- Spark Streaming ingestion with explicit schema validation.\n- Benchmark-driven choice between streaming and file-based ingestion.\n- Unified monitoring for latency and error rates.\n\n## Decisions and tradeoffs\n- We traded some Lambda convenience for predictable throughput and cost.\n- We used benchmarking to choose the right ingestion mode per market.\n- We treated data quality as a first-class output, not an afterthought.\n\n## What worked\n- Stable latency under peak load.\n- Clear cost breakpoints for streaming vs file-based options.\n- Fewer quality regressions due to consistent validation.\n\n## What I would change\n- I would invest in replay tooling earlier to speed incident recovery.\n\n## Key takeaways\n- Streaming is not always cheaper, but it is more predictable when designed well.\n- Benchmarking is essential before committing to a data ingestion strategy.\n- Data quality improves when validation is part of the pipeline, not a downstream check.",
      "file": "streaming-ingestion.md"
    }
  ]
} as const;
